{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device      = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The size of each batch.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Training data split\n",
    "trainset    = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Testing data split\n",
    "testset     = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader  = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "classes     = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, net, criterion):\n",
    "    device      = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    start = time.time()\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        stop = time.time()\n",
    "        print('[Time: %4.3fs][epoch %3d] Loss: %.3f | Acc: %.3f%% (%d/%d)'%(stop - start, epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    # Return the accuracy\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "import time\n",
    "\n",
    "class Runtime_Info(object):\n",
    "    def __init__(self, acc_progression, all_losses, runtime, pre_time, pro_time, com_time, rounds, accuracy, network):\n",
    "        self.acc_progression  = acc_progression\n",
    "        self.all_losses       = all_losses\n",
    "        self.runtime          = runtime\n",
    "        self.pre_time         = pre_time\n",
    "        self.pro_time         = pro_time\n",
    "        self.com_time         = com_time\n",
    "        self.rounds           = rounds\n",
    "        self.accuracy         = accuracy\n",
    "        self.network          = network\n",
    "        \n",
    "    def __cmp__(self, other):\n",
    "        if hasattr(other, 'accuracy'):\n",
    "            return other.accuracy.__cmp__(self.accuracy)\n",
    "\n",
    "def Run_Test(Batch_Size, Learning_Rate, Batch_Loops, max_epochs):\n",
    "    # Prepare the dataset:\n",
    "    torch.cuda.empty_cache()\n",
    "    Minutes = 120\n",
    "    \n",
    "    # Create the network and send it to the GPU\n",
    "    device      = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net         = ResNet18().cuda()\n",
    "    net         = net.to(device)\n",
    "    #net             = Net(Network_Size).cuda()\n",
    "    #net             = net.to(device)\n",
    "    #print(\"Cuda available? \" + str(next(net.parameters()).is_cuda))\n",
    "\n",
    "    # Criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=Learning_Rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    # Network parameters\n",
    "    #max_epochs         = 10000\n",
    "    running_loss       = 0.0\n",
    "    preprocessing_time = 0. \n",
    "    processing_time    = 0.\n",
    "    i                  = 0\n",
    "    prInterval         = 10\n",
    "    all_losses         = []\n",
    "    epoch_times        = []\n",
    "    acc_progression    = []\n",
    "\n",
    "    # Event timing\n",
    "    start = 0\n",
    "    stop  = 0\n",
    "\n",
    "    # Epoch timing\n",
    "    beginning = 0\n",
    "    end       = 0\n",
    "\n",
    "    # Timing data for analysis\n",
    "    runtime   = 0\n",
    "    pre_time  = 0\n",
    "    pro_time  = 0\n",
    "    com_time  = 0\n",
    "    rounds    = 0\n",
    "\n",
    "    # Full training loop\n",
    "    runtime   = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        print(\"[Epoch %d duration: %3.3fs][Total duration: %3.3fs]\\n\"% (epoch, float(end - beginning), float(time.time() - runtime)))\n",
    "        beginning = time.time()\n",
    "    \n",
    "        for local_batch, local_labels in trainloader:\n",
    "            start = time.time()\n",
    "            inputs, labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "            stop  = time.time()\n",
    "            preprocessing_time += stop - start\n",
    "            start = time.time()\n",
    "        \n",
    "            # Force five walks over this batch before evicting from GPU\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i in range(Batch_Loops):\n",
    "                outputs = net(inputs)\n",
    "                loss    = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            del inputs, labels\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # Record the loss for visualization of training\n",
    "            all_losses.append(loss.item())\n",
    "            i += 1\n",
    "        \n",
    "            stop = time.time()\n",
    "            processing_time += stop - start\n",
    "        \n",
    "        end       = time.time()\n",
    "        comm_time = (end- beginning) - processing_time - preprocessing_time\n",
    "        print('[Time: %4.3fs][epoch %3d] compute time %.3f, comm. overhead %.3f, epoch tot. time %3.3f' %\n",
    "                (stop - beginning, epoch + 1, processing_time, comm_time, end - beginning) )\n",
    "        accuracy       = test(epoch, net, criterion)\n",
    "    \n",
    "        # Create a tupe of (Epoch Time, Processing Time, Preprocessing Time, Communication time, Overall time)\n",
    "        epoch_times.append((end - beginning, processing_time, preprocessing_time, comm_time, end - runtime))\n",
    "    \n",
    "        # Keep record of the total portion of runtime each section requires\n",
    "        pro_time += processing_time\n",
    "        com_time += comm_time\n",
    "        pre_time += preprocessing_time\n",
    "        rounds   += 1\n",
    "        #running_loss       = 0.0\n",
    "        preprocessing_time  = 0\n",
    "        processing_time     = 0\n",
    "        #i                  = 0\n",
    "    \n",
    "        # Record accuracy for visualization of accuracy progression\n",
    "        acc_progression.append(accuracy)\n",
    "        if accuracy > 90 or float(time.time() - runtime) > (60 * Minutes):\n",
    "            break\n",
    "\n",
    "    # Calculate the final runtime\n",
    "    total_time = time.time() - runtime\n",
    "    accuracy   = max(acc_progression)\n",
    "    print(\"Final accuracy over %d Epochs: %3.3f, Elapsed time: %3.3f\"%(rounds, accuracy, total_time))\n",
    "    return Runtime_Info(acc_progression, all_losses, runtime, pre_time, pro_time, com_time, rounds, accuracy, net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 duration: 0.000s][Total duration: 0.000s]\n",
      "\n",
      "[Time: 87.241s][epoch   1] compute time 84.398, comm. overhead 2.632, epoch tot. time 87.241\n",
      "[Time: 5.521s][epoch   0] Loss: 1.418 | Acc: 50.020% (5002/10000)\n",
      "[Epoch 1 duration: 87.241s][Total duration: 92.765s]\n",
      "\n",
      "[Time: 86.999s][epoch   2] compute time 84.112, comm. overhead 2.660, epoch tot. time 86.999\n",
      "[Time: 5.529s][epoch   1] Loss: 1.207 | Acc: 56.530% (5653/10000)\n",
      "[Epoch 2 duration: 86.999s][Total duration: 185.296s]\n",
      "\n",
      "[Time: 86.694s][epoch   3] compute time 83.813, comm. overhead 2.670, epoch tot. time 86.694\n",
      "[Time: 5.519s][epoch   2] Loss: 1.096 | Acc: 60.990% (6099/10000)\n",
      "[Epoch 3 duration: 86.694s][Total duration: 277.512s]\n",
      "\n",
      "[Time: 87.106s][epoch   4] compute time 84.222, comm. overhead 2.659, epoch tot. time 87.106\n",
      "[Time: 5.507s][epoch   3] Loss: 1.045 | Acc: 61.170% (6117/10000)\n",
      "[Epoch 4 duration: 87.106s][Total duration: 370.129s]\n",
      "\n",
      "[Time: 86.541s][epoch   5] compute time 83.667, comm. overhead 2.664, epoch tot. time 86.541\n",
      "[Time: 5.512s][epoch   4] Loss: 0.919 | Acc: 66.690% (6669/10000)\n",
      "[Epoch 5 duration: 86.541s][Total duration: 462.185s]\n",
      "\n",
      "[Time: 86.986s][epoch   6] compute time 84.065, comm. overhead 2.698, epoch tot. time 86.986\n",
      "[Time: 5.637s][epoch   5] Loss: 0.862 | Acc: 70.050% (7005/10000)\n",
      "[Epoch 6 duration: 86.986s][Total duration: 554.811s]\n",
      "\n",
      "[Time: 87.494s][epoch   7] compute time 84.394, comm. overhead 2.860, epoch tot. time 87.495\n",
      "[Time: 5.794s][epoch   6] Loss: 0.812 | Acc: 71.740% (7174/10000)\n",
      "[Epoch 7 duration: 87.495s][Total duration: 648.103s]\n",
      "\n",
      "[Time: 87.804s][epoch   8] compute time 84.774, comm. overhead 2.787, epoch tot. time 87.805\n",
      "[Time: 5.519s][epoch   7] Loss: 0.781 | Acc: 72.410% (7241/10000)\n",
      "[Epoch 8 duration: 87.805s][Total duration: 741.429s]\n",
      "\n",
      "[Time: 86.859s][epoch   9] compute time 83.959, comm. overhead 2.667, epoch tot. time 86.859\n",
      "[Time: 5.491s][epoch   8] Loss: 0.749 | Acc: 73.730% (7373/10000)\n",
      "[Epoch 9 duration: 86.859s][Total duration: 833.782s]\n",
      "\n",
      "[Time: 87.079s][epoch  10] compute time 84.210, comm. overhead 2.650, epoch tot. time 87.079\n",
      "[Time: 5.639s][epoch   9] Loss: 0.701 | Acc: 75.340% (7534/10000)\n",
      "[Epoch 10 duration: 87.079s][Total duration: 926.505s]\n",
      "\n",
      "[Time: 86.578s][epoch  11] compute time 83.710, comm. overhead 2.652, epoch tot. time 86.578\n",
      "[Time: 5.508s][epoch  10] Loss: 0.676 | Acc: 76.270% (7627/10000)\n",
      "[Epoch 11 duration: 86.578s][Total duration: 1018.594s]\n",
      "\n",
      "[Time: 87.041s][epoch  12] compute time 84.156, comm. overhead 2.663, epoch tot. time 87.041\n",
      "[Time: 5.523s][epoch  11] Loss: 0.664 | Acc: 76.980% (7698/10000)\n",
      "[Epoch 12 duration: 87.041s][Total duration: 1111.162s]\n",
      "\n",
      "[Time: 86.590s][epoch  13] compute time 83.688, comm. overhead 2.682, epoch tot. time 86.590\n",
      "[Time: 5.527s][epoch  12] Loss: 0.643 | Acc: 77.390% (7739/10000)\n",
      "[Epoch 13 duration: 86.590s][Total duration: 1203.284s]\n",
      "\n",
      "[Time: 87.579s][epoch  14] compute time 84.664, comm. overhead 2.666, epoch tot. time 87.580\n",
      "[Time: 5.557s][epoch  13] Loss: 0.589 | Acc: 79.730% (7973/10000)\n",
      "[Epoch 14 duration: 87.580s][Total duration: 1296.424s]\n",
      "\n",
      "[Time: 86.689s][epoch  15] compute time 83.798, comm. overhead 2.673, epoch tot. time 86.689\n",
      "[Time: 5.510s][epoch  14] Loss: 0.578 | Acc: 79.540% (7954/10000)\n",
      "[Epoch 15 duration: 86.689s][Total duration: 1388.626s]\n",
      "\n",
      "[Time: 87.340s][epoch  16] compute time 84.428, comm. overhead 2.692, epoch tot. time 87.340\n",
      "[Time: 5.510s][epoch  15] Loss: 0.583 | Acc: 80.010% (8001/10000)\n",
      "[Epoch 16 duration: 87.340s][Total duration: 1481.479s]\n",
      "\n",
      "[Time: 86.769s][epoch  17] compute time 83.878, comm. overhead 2.663, epoch tot. time 86.769\n",
      "[Time: 5.504s][epoch  16] Loss: 0.543 | Acc: 81.170% (8117/10000)\n",
      "[Epoch 17 duration: 86.769s][Total duration: 1573.756s]\n",
      "\n",
      "[Time: 87.054s][epoch  18] compute time 84.176, comm. overhead 2.641, epoch tot. time 87.054\n",
      "[Time: 5.599s][epoch  17] Loss: 0.525 | Acc: 82.370% (8237/10000)\n",
      "[Epoch 18 duration: 87.054s][Total duration: 1666.414s]\n",
      "\n",
      "[Time: 86.656s][epoch  19] compute time 83.743, comm. overhead 2.683, epoch tot. time 86.656\n",
      "[Time: 5.517s][epoch  18] Loss: 0.544 | Acc: 81.350% (8135/10000)\n",
      "[Epoch 19 duration: 86.656s][Total duration: 1758.589s]\n",
      "\n",
      "[Time: 87.366s][epoch  20] compute time 84.438, comm. overhead 2.681, epoch tot. time 87.366\n",
      "[Time: 5.541s][epoch  19] Loss: 0.483 | Acc: 83.400% (8340/10000)\n",
      "[Epoch 20 duration: 87.366s][Total duration: 1851.499s]\n",
      "\n",
      "[Time: 86.958s][epoch  21] compute time 84.017, comm. overhead 2.718, epoch tot. time 86.959\n",
      "[Time: 5.543s][epoch  20] Loss: 0.496 | Acc: 83.320% (8332/10000)\n",
      "[Epoch 21 duration: 86.959s][Total duration: 1944.004s]\n",
      "\n",
      "[Time: 86.920s][epoch  22] compute time 84.008, comm. overhead 2.678, epoch tot. time 86.921\n",
      "[Time: 5.537s][epoch  21] Loss: 0.513 | Acc: 82.910% (8291/10000)\n",
      "[Epoch 22 duration: 86.921s][Total duration: 2036.465s]\n",
      "\n",
      "[Time: 86.810s][epoch  23] compute time 83.873, comm. overhead 2.715, epoch tot. time 86.811\n",
      "[Time: 5.506s][epoch  22] Loss: 0.494 | Acc: 83.320% (8332/10000)\n",
      "[Epoch 23 duration: 86.811s][Total duration: 2128.784s]\n",
      "\n",
      "[Time: 87.006s][epoch  24] compute time 84.118, comm. overhead 2.661, epoch tot. time 87.007\n",
      "[Time: 5.542s][epoch  23] Loss: 0.515 | Acc: 82.840% (8284/10000)\n",
      "[Epoch 24 duration: 87.007s][Total duration: 2221.336s]\n",
      "\n",
      "[Time: 86.801s][epoch  25] compute time 83.943, comm. overhead 2.639, epoch tot. time 86.802\n",
      "[Time: 5.517s][epoch  24] Loss: 0.478 | Acc: 83.660% (8366/10000)\n",
      "[Epoch 25 duration: 86.802s][Total duration: 2313.658s]\n",
      "\n",
      "[Time: 87.073s][epoch  26] compute time 84.188, comm. overhead 2.657, epoch tot. time 87.073\n",
      "[Time: 5.531s][epoch  25] Loss: 0.476 | Acc: 83.800% (8380/10000)\n",
      "[Epoch 26 duration: 87.073s][Total duration: 2406.266s]\n",
      "\n",
      "[Time: 86.631s][epoch  27] compute time 83.752, comm. overhead 2.660, epoch tot. time 86.631\n",
      "[Time: 5.521s][epoch  26] Loss: 0.493 | Acc: 83.570% (8357/10000)\n",
      "[Epoch 27 duration: 86.631s][Total duration: 2498.422s]\n",
      "\n",
      "[Time: 87.031s][epoch  28] compute time 84.131, comm. overhead 2.670, epoch tot. time 87.031\n",
      "[Time: 5.523s][epoch  27] Loss: 0.411 | Acc: 86.380% (8638/10000)\n",
      "[Epoch 28 duration: 87.031s][Total duration: 2590.980s]\n",
      "\n",
      "[Time: 86.790s][epoch  29] compute time 83.926, comm. overhead 2.652, epoch tot. time 86.790\n",
      "[Time: 5.529s][epoch  28] Loss: 0.428 | Acc: 85.540% (8554/10000)\n",
      "[Epoch 29 duration: 86.790s][Total duration: 2683.302s]\n",
      "\n",
      "[Time: 87.009s][epoch  30] compute time 84.075, comm. overhead 2.706, epoch tot. time 87.009\n",
      "[Time: 5.528s][epoch  29] Loss: 0.462 | Acc: 84.770% (8477/10000)\n",
      "[Epoch 30 duration: 87.009s][Total duration: 2775.843s]\n",
      "\n",
      "[Time: 86.566s][epoch  31] compute time 83.682, comm. overhead 2.660, epoch tot. time 86.566\n",
      "[Time: 5.513s][epoch  30] Loss: 0.461 | Acc: 84.760% (8476/10000)\n",
      "[Epoch 31 duration: 86.566s][Total duration: 2867.925s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3d4041e81369>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRun_Test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatch_Size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLearning_Rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatch_Loops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-6dab48363725>\u001b[0m in \u001b[0;36mRun_Test\u001b[1;34m(Batch_Size, Learning_Rate, Batch_Loops, max_epochs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;31m# Record the loss for visualization of training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Run_Test(Batch_Size = 32, Learning_Rate = .001, Batch_Loops = 1, max_epochs = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
